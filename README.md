# Absolute Zero Reasoner - Coder 3b Q4_K_M (GGUF)

This repository contains the `andrewzh_Absolute_Zero_Reasoner-Coder-3b-Q4_K_M.gguf` model file. This file is intended for use with GGUF-compatible inference engines.

## About

- **Model Name:** Absolute Zero Reasoner - Coder 3b Q4_K_M
- **Format:** GGUF
- **Author:** andrewzh
- **Description:**
  This is a quantized language model file in GGUF format, suitable for use with various local LLM inference tools. Please refer to the documentation of your inference engine for compatibility and usage instructions.

## Usage

1. Download this repository or the model file directly.
2. Use with your preferred GGUF-compatible inference engine (e.g., llama.cpp, koboldcpp, etc.).

Example command (llama.cpp):
```sh
./main -m andrewzh_Absolute_Zero_Reasoner-Coder-3b-Q4_K_M.gguf
```

## Requirements

- GGUF-compatible inference engine (e.g., llama.cpp, koboldcpp)
- Sufficient system RAM and disk space for the model file

## License

Please refer to the LICENSE file or the model author's terms for usage and distribution rights.

## Acknowledgements

- Model by andrewzh
- GGUF format by the llama.cpp community

---

For questions or issues, please open an issue on this repository.
# llama-models
